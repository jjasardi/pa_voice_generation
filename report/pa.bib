@online{cbsnews2023voice,
  title   = {Cybercriminals are using AI voice cloning tools to dupe victims},
  author  = {{CSBS News}},
  year    = 2023,
  url     = {https://www.cbsnews.com/news/ai-scam-voice-cloning-rising/},
  urldate = {2023-10-08}
}

@online{arstechnica2023valle,
  title   = {Microsoft’s new AI can simulate anyone’s voice with 3 seconds of audio},
  author  = {{Ars Technica}},
  year    = 2023,
  url     = {https://arstechnica.com/information-technology/2023/01/microsofts-new-ai-can-simulate-anyones-voice-with-3-seconds-of-audio/},
  urldate = {2023-11-07}
}

@online{vice2023bank,
  title   = {How I Broke Into a Bank Account With an AI-Generated Voice},
  author  = {{Joseph Cox}},
  year    = 2023,
  url     = {https://www.vice.com/en/article/dy7axa/how-i-broke-into-a-bank-account-with-an-ai-generated-voice},
  urldate = {2023-12-19}
}

@article{wu2015spoofing,
  title     = {Spoofing and countermeasures for speaker verification: A survey},
  author    = {Wu, Zhizheng and Evans, Nicholas and Kinnunen, Tomi and Yamagishi, Junichi and Alegre, Federico and Li, Haizhou},
  journal   = {speech communication},
  volume    = {66},
  pages     = {130--153},
  year      = {2015},
  publisher = {Elsevier}
}

@article{yan2022survey,
  title     = {A survey on voice assistant security: Attacks and countermeasures},
  author    = {Yan, Chen and Ji, Xiaoyu and Wang, Kai and Jiang, Qinhong and Jin, Zizhi and Xu, Wenyuan},
  journal   = {ACM Computing Surveys},
  volume    = {55},
  number    = {4},
  pages     = {1--36},
  year      = {2022},
  publisher = {ACM New York, NY}
}

@article{yi2023audio,
  title   = {Audio Deepfake Detection: A Survey},
  author  = {Yi, Jiangyan and Wang, Chenglong and Tao, Jianhua and Zhang, Xiaohui and Zhang, Chu Yuan and Zhao, Yan},
  journal = {arXiv preprint arXiv:2308.14970},
  year    = {2023}
}

@article{hannun2014deep,
  title   = {Deep speech: Scaling up end-to-end speech recognition},
  author  = {Hannun, Awni and Case, Carl and Casper, Jared and Catanzaro, Bryan and Diamos, Greg and Elsen, Erich and Prenger, Ryan and Satheesh, Sanjeev and Sengupta, Shubho and Coates, Adam and others},
  journal = {arXiv preprint arXiv:1412.5567},
  year    = {2014}
}

@article{wang2301neural,
  title   = {Neural Codec Language Models Are Zero-Shot Text to Speech Synthesizers, 2023},
  author  = {Wang, Chengyi and Chen, Sanyuan and Wu, Yu and Zhang, Ziqiang and Zhou, Long and Liu, Shujie and Chen, Zhuo and Liu, Yanqing and Wang, Huaming and Li, Jinyu and others},
  journal = {URL: https://arxiv. org/abs/2301.02111. doi: doi},
  volume  = {10}
}

@inproceedings{shen2018natural,
  title        = {Natural tts synthesis by conditioning wavenet on mel spectrogram predictions},
  author       = {Shen, Jonathan and Pang, Ruoming and Weiss, Ron J and Schuster, Mike and Jaitly, Navdeep and Yang, Zongheng and Chen, Zhifeng and Zhang, Yu and Wang, Yuxuan and Skerrv-Ryan, Rj and others},
  booktitle    = {2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages        = {4779--4783},
  year         = {2018},
  organization = {IEEE}
}

@article{ren2019fastspeech,
  title   = {Fastspeech: Fast, robust and controllable text to speech},
  author  = {Ren, Yi and Ruan, Yangjun and Tan, Xu and Qin, Tao and Zhao, Sheng and Zhao, Zhou and Liu, Tie-Yan},
  journal = {Advances in neural information processing systems},
  volume  = {32},
  year    = {2019}
}

@inproceedings{li2019neural,
  title     = {Neural speech synthesis with transformer network},
  author    = {Li, Naihan and Liu, Shujie and Liu, Yanqing and Zhao, Sheng and Liu, Ming},
  booktitle = {Proceedings of the AAAI conference on artificial intelligence},
  volume    = {33},
  number    = {01},
  pages     = {6706--6713},
  year      = {2019}
}

@article{arik2018neural,
  title   = {Neural voice cloning with a few samples},
  author  = {Arik, Sercan and Chen, Jitong and Peng, Kainan and Ping, Wei and Zhou, Yanqi},
  journal = {Advances in neural information processing systems},
  volume  = {31},
  year    = {2018}
}

@inproceedings{casanova2022yourtts,
  title        = {Yourtts: Towards zero-shot multi-speaker tts and zero-shot voice conversion for everyone},
  author       = {Casanova, Edresson and Weber, Julian and Shulby, Christopher D and Junior, Arnaldo Candido and G{\"o}lge, Eren and Ponti, Moacir A},
  booktitle    = {International Conference on Machine Learning},
  pages        = {2709--2720},
  year         = {2022},
  organization = {PMLR}
}

@inbook{Tan2023textanalysis,
  author    = {Tan, Xu},
  title     = {Text Analyses},
  booktitle = {Neural Text-to-Speech Synthesis},
  year      = {2023},
  publisher = {Springer Nature Singapore},
  address   = {Singapore},
  pages     = {67--80},
  abstract  = {Through text analyses, we can transform input text into linguistic features, which contain rich information about pronunciation and prosody that can ease speech synthesis. Text analyses consist of several components: (1) text processing, which processes raw text from documents, normalizes the text from the written form into spoken form, and conducts some linguistic analyses; (2) phonetic analysis, which converts text into phonetic symbols, including polyphone disambiguation and grapheme-to-phoneme conversion; (3) prosodic analysis, which analyzes some prosodic features such as pitch, duration, loudness, stress, and pauses. In this chapter, we first introduce these components in the first three sections and then discuss the development of text analysis in TTS in the last section.},
  isbn      = {978-981-99-0827-1},
  doi       = {10.1007/978-981-99-0827-1_4},
  url       = {https://doi.org/10.1007/978-981-99-0827-1_4}
}

@inbook{Tan2023vocoder,
  author    = {Tan, Xu},
  title     = {Vocoders},
  booktitle = {Neural Text-to-Speech Synthesis},
  year      = {2023},
  publisher = {Springer Nature Singapore},
  address   = {Singapore},
  pages     = {101--114},
  abstract  = {In this chapter, we introduce vocoders, which generate waveforms from acoustic features or directly from linguistic features. With the development of TTS, different kinds of vocoders have been adopted, including the vocoders in statistical parametric speech synthesis (SPSS), and neural network-based vocoders. We first view vocoders from a historic perspective, covering vocoders in SPSS and neural TTS, and then introduce the vocoders in neural TTS, mainly from the perspective of different deep generative models used.},
  isbn      = {978-981-99-0827-1},
  doi       = {10.1007/978-981-99-0827-1_6},
  url       = {https://doi.org/10.1007/978-981-99-0827-1_6}
}

@inbook{Tan2023fullyendtoend,
  author    = {Tan, Xu},
  title     = {Fully End-to-End TTS},
  booktitle = {Neural Text-to-Speech Synthesis},
  year      = {2023},
  publisher = {Springer Nature Singapore},
  address   = {Singapore},
  pages     = {115--122},
  abstract  = {Fully end-to-end TTS models can generate speech waveforms from character or phoneme sequences directly. However, there are big challenges to training TTS models in an end-to-end way, mainly due to the different modalities between text and speech waveform, as well as the huge length mismatch between character/phoneme sequence and waveform sequence. Thus, the development of end-to-end TTS is progressive. In this chapter, we first review the progressively end-to-end process in TTS from a historical perspective and then introduce some fully end-to-end TTS models.},
  isbn      = {978-981-99-0827-1},
  doi       = {10.1007/978-981-99-0827-1_7},
  url       = {https://doi.org/10.1007/978-981-99-0827-1_7}
}


@article{sproat2001normalization,
  title     = {Normalization of non-standard words},
  author    = {Sproat, Richard and Black, Alan W and Chen, Stanley and Kumar, Shankar and Ostendorf, Mari and Richards, Christina D},
  journal   = {Computer speech \& language},
  volume    = {15},
  number    = {3},
  pages     = {287--333},
  year      = {2001},
  publisher = {Elsevier}
}

@article{sun2019token,
  title   = {Token-level ensemble distillation for grapheme-to-phoneme conversion},
  author  = {Sun, Hao and Tan, Xu and Gan, Jun-Wei and Liu, Hongzhi and Zhao, Sheng and Qin, Tao and Liu, Tie-Yan},
  journal = {arXiv preprint arXiv:1904.03446},
  year    = {2019}
}

@inbook{acoustic2023models,
  author    = {Tan, Xu},
  title     = {Acoustic Models},
  booktitle = {Neural Text-to-Speech Synthesis},
  year      = {2023},
  publisher = {Springer Nature Singapore},
  address   = {Singapore},
  pages     = {81--100},
  abstract  = {In this chapter, we introduce acoustic models, which generate acoustic features from linguistic features or directly from phonemes or characters. With the development of TTS, different kinds of acoustic models have been adopted, including the early hidden Markov models and deep neural networks in statistical parametric speech synthesis, and then the sequence-to-sequence models based on an encoder-attention-decoder framework (including RNN, CNN, and Transformer), and the latest feed-forward models (CNN or Transformer) and advanced generative models (GAN, Flow, VAE, and Diffusion).},
  isbn      = {978-981-99-0827-1},
  doi       = {10.1007/978-981-99-0827-1_5},
  url       = {https://doi.org/10.1007/978-981-99-0827-1_5}
}

@inproceedings{yoshimura1999simultaneous,
  title     = {Simultaneous modeling of spectrum, pitch and duration in HMM-based speech synthesis},
  author    = {Yoshimura, Takayoshi and Tokuda, Keiichi and Masuko, Takashi and Kobayashi, Takao and Kitamura, Tadashi},
  booktitle = {Sixth European conference on speech communication and technology},
  year      = {1999}
}

@article{tokuda2013speech,
  title     = {Speech synthesis based on hidden Markov models},
  author    = {Tokuda, Keiichi and Nankaku, Yoshihiko and Toda, Tomoki and Zen, Heiga and Yamagishi, Junichi and Oura, Keiichiro},
  journal   = {Proceedings of the IEEE},
  volume    = {101},
  number    = {5},
  pages     = {1234--1252},
  year      = {2013},
  publisher = {IEEE}
}

@article{zen2015acoustic,
  title  = {Acoustic modeling in statistical parametric speech synthesis-from HMM to LSTM-RNN},
  author = {Zen, Heiga},
  year   = {2015}
}

@article{wang2017tacotron,
  title   = {Tacotron: Towards end-to-end speech synthesis},
  author  = {Wang, Yuxuan and Skerry-Ryan, RJ and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhifeng and Bengio, Samy and others},
  journal = {arXiv preprint arXiv:1703.10135},
  year    = {2017}
}


@inproceedings{arik2017deep,
  title        = {Deep voice: Real-time neural text-to-speech},
  author       = {Ar{\i}k, Sercan {\"O} and Chrzanowski, Mike and Coates, Adam and Diamos, Gregory and Gibiansky, Andrew and Kang, Yongguo and Li, Xian and Miller, John and Ng, Andrew and Raiman, Jonathan and others},
  booktitle    = {International conference on machine learning},
  pages        = {195--204},
  year         = {2017},
  organization = {PMLR}
}

@inproceedings{ren2021fastspeech,
  title     = {FastSpeech 2: Fast and High-Quality End-to-End Text to Speech},
  author    = {Yi Ren and Chenxu Hu and Xu Tan and Tao Qin and Sheng Zhao and Zhou Zhao and Tie-Yan Liu},
  booktitle = {International Conference on Learning Representations},
  year      = {2021},
  url       = {https://openreview.net/forum?id=piLPYqxtWuA}
}

@article{betker2023better,
  title   = {Better speech synthesis through scaling},
  author  = {Betker, James},
  journal = {arXiv preprint arXiv:2305.07243},
  year    = {2023}
}

@software{betker_tortoise_2022,
  title    = {{TorToiSe} text-to-speech},
  rights   = {Apache-2.0},
  url      = {https://github.com/neonbjb/tortoise-tts},
  abstract = {A multi-voice {TTS} system trained with an emphasis on quality},
  version  = {2.0},
  author   = {Betker, James},
  urldate  = {2023-11-29},
  date     = {2022-04},
  note     = {original-date: 2022-01-28T04:33:15Z}
}

@article{jemine2019master,
  title     = {Master thesis: Real-time voice cloning},
  author    = {Jemine, Corentin and others},
  year      = {2019},
  publisher = {Universit{\'e} de Li{\`e}ge, Li{\`e}ge, Belgique}
}

@article{jia2018transfer,
  title   = {Transfer learning from speaker verification to multispeaker text-to-speech synthesis},
  author  = {Jia, Ye and Zhang, Yu and Weiss, Ron and Wang, Quan and Shen, Jonathan and Ren, Fei and Nguyen, Patrick and Pang, Ruoming and Lopez Moreno, Ignacio and Wu, Yonghui and others},
  journal = {Advances in neural information processing systems},
  volume  = {31},
  year    = {2018}
}

@inproceedings{wan2018generalized,
  title        = {Generalized end-to-end loss for speaker verification},
  author       = {Wan, Li and Wang, Quan and Papir, Alan and Moreno, Ignacio Lopez},
  booktitle    = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages        = {4879--4883},
  year         = {2018},
  organization = {IEEE}
}

@misc{valle,
  author = {Feiteng Li},
  title  = {VALL-E: A neural codec language model},
  year   = {2023},
  url    = {http://github.com/lifeiteng/vall-e}
}

@software{niu_vall-e_2023,
  title    = {{VALL}-E},
  rights   = {{MIT}},
  url      = {https://github.com/enhuiz/vall-e},
  abstract = {An unofficial {PyTorch} implementation of the audio {LM} {VALL}-E},
  author   = {Niu, Zhe},
  urldate  = {2023-12-05},
  date     = {2023-12-01},
  note     = {original-date: 2023-01-11T11:32:21Z},
  keywords = {audio-lm, pytorch, text-to-speech, tts, vall-e, valle}
}

@software{songting_vall-e_2023,
  title      = {{VALL}-E X: Multilingual Text-to-Speech Synthesis and Voice Cloning ��},
  rights     = {{MIT}},
  url        = {https://github.com/Plachtaa/VALL-E-X},
  shorttitle = {{VALL}-E X},
  abstract   = {An open source implementation of Microsoft's {VALL}-E X zero-shot {TTS} model. Demo is available in https://plachtaa.github.io},
  author     = {Songting},
  urldate    = {2023-12-05},
  date       = {2023-12-05},
  note       = {original-date: 2023-07-29T07:10:10Z},
  keywords   = {emotional-speech, gpt, text-to-speech, transformer-architecture, tts, vall-e, voice-clone}
}

@article{zhang2023speak,
  title   = {Speak foreign languages with your own voice: Cross-lingual neural codec language modeling},
  author  = {Zhang, Ziqiang and Zhou, Long and Wang, Chengyi and Chen, Sanyuan and Wu, Yu and Liu, Shujie and Chen, Zhuo and Liu, Yanqing and Wang, Huaming and Li, Jinyu and others},
  journal = {arXiv preprint arXiv:2303.03926},
  year    = {2023}
}

@article{defossez2022high,
  title   = {High fidelity neural audio compression},
  author  = {D{\'e}fossez, Alexandre and Copet, Jade and Synnaeve, Gabriel and Adi, Yossi},
  journal = {arXiv preprint arXiv:2210.13438},
  year    = {2022}
}

@online{noauthor_demo_nodate,
  title      = {Demo of reproduced {VALL}-E X},
  url        = {https://plachtaa.github.io/},
  titleaddon = {{VALL}-E},
  urldate    = {2023-12-06}
}

@article{jang2021univnet,
  title   = {Univnet: A neural vocoder with multi-resolution spectrogram discriminators for high-fidelity waveform generation},
  author  = {Jang, Won and Lim, Dan and Yoon, Jaesam and Kim, Bongwan and Kim, Juntae},
  journal = {arXiv preprint arXiv:2106.07889},
  year    = {2021}
}

@article{oord2016wavenet,
  title   = {Wavenet: A generative model for raw audio},
  author  = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  journal = {arXiv preprint arXiv:1609.03499},
  year    = {2016}
}

@online{betker_tortoise_hugging_face,
  title    = {Tortoise Tts - a Hugging Face Space by Manmay},
  url      = {https://huggingface.co/spaces/Manmay/tortoise-tts},
  abstract = {Discover amazing {ML} apps made by the community},
  author   = {Betker, James},
  urldate  = {2023-12-12}
}

@online{mullis_afiaka87tortoise-tts_nodate,
  title    = {afiaka87/tortoise-tts – Run with an {API} on Replicate},
  url      = {https://replicate.com/afiaka87/tortoise-tts},
  abstract = {Generate speech from text, clone voices from mp3 files. From James Betker {AKA} "neonbjb".},
  author   = {Mullis, Clay},
  urldate  = {2023-12-12}
}

@online{vallex_hugging_face,
  title    = {VALL E X - a Hugging Face Space by Plachta},
  url      = {https://huggingface.co/spaces/Plachta/VALL-E-X},
  abstract = {Discover amazing {ML} apps made by the community},
  author   = {Songting},
  urldate  = {2023-12-12}
}

@article{siuzdak2023vocos,
  title   = {Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis},
  author  = {Siuzdak, Hubert},
  journal = {arXiv preprint arXiv:2306.00814},
  year    = {2023}
}


@software{jemine_corentinjreal-time-voice-cloning_2023,
  title    = {{CorentinJ}/Real-Time-Voice-Cloning},
  url      = {https://github.com/CorentinJ/Real-Time-Voice-Cloning},
  abstract = {Clone a voice in 5 seconds to generate arbitrary speech in real-time},
  author   = {Jemine, Corentin},
  urldate  = {2023-12-14},
  date     = {2023-12-14},
  note     = {original-date: 2019-05-26T08:56:15Z},
  keywords = {deep-learning, python, pytorch, tensorflow, tts, voice-cloning}
}

  @misc{enwiki:1189644011,
  author = {{Wikipedia contributors}},
  title  = {Speech synthesis --- {Wikipedia}{,} The Free Encyclopedia},
  year   = {2023},
  url    = {https://en.wikipedia.org/w/index.php?title=Speech_synthesis&oldid=1189644011},
  note   = {[Online; accessed 19-December-2023]}
}

@article{maurer2000diffie,
  title     = {The diffie--hellman protocol},
  author    = {Maurer, Ueli M and Wolf, Stefan},
  journal   = {Designs, Codes and Cryptography},
  volume    = {19},
  number    = {2-3},
  pages     = {147--171},
  year      = {2000},
  publisher = {Springer}
}